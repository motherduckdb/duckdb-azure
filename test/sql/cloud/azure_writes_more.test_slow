# name: test/sql/cloud/azure_writes_more.test_slow
# description: test azure extension writes
# group: [cloud]

require azure

require parquet

require-env ABFSS_TEMP_DIR

require-env ABFSS_STORAGE_ACCOUNT

require-env AZ_TEMP_DIR

require-env AZ_STORAGE_ACCOUNT

require-env AZURE_AUTH_ENV 1

statement ok
CREATE OR REPLACE SECRET abfss_authn (
	TYPE AZURE,
	PROVIDER CREDENTIAL_CHAIN,
	ACCOUNT_NAME '${ABFSS_STORAGE_ACCOUNT}',
	SCOPE 'abfss://'
);

statement ok
CREATE OR REPLACE SECRET az_authn (
	TYPE AZURE,
	PROVIDER CREDENTIAL_CHAIN,
	ACCOUNT_NAME '${AZ_STORAGE_ACCOUNT}',
	SCOPE 'az://'
);

## Some small writes - 10k, via 100 threads
concurrentloop i 0 100

statement ok
COPY (
	SELECT id: ('${i}'::INT * 100) + generate_series
	FROM generate_series(0, 99)
)
TO 'az://duckdblabs-write-testing/extension/azure/many/write-concurrent-${i}.csv';

endloop

# Read it back as a sum: n*(n+1) / 2
query I 
SELECT sum(id) FROM 'az://duckdblabs-write-testing/extension/azure/many/write-concurrent-*.csv';
----
49995000


## Some small writes - 10k, via 100 threads
concurrentloop i 0 100

statement ok
COPY (
	SELECT id: ('${i}'::INT * 100) + generate_series
	FROM generate_series(0, 99)
)
TO 'abfss://duckdblabs-write-testing/extension/azure/many/write-concurrent-${i}.csv';

endloop

# Read it back as a sum: n*(n+1) / 2
query I 
SELECT sum(id) FROM 'abfss://duckdblabs-write-testing/extension/azure/many/write-concurrent-*.csv';
----
49995000

foreach proto abfss az

## Many small writes - 1k
foreach i 0 1 2 3 4 5 6 7 8 9

foreach j 0 1 2 3 4 5 6 7 8 9

foreach k 0 1 2 3 4 5 6 7 8 9

statement ok
COPY (SELECT id: '${i}${j}${k}'::INT) 
TO '${proto}://duckdblabs-write-testing/extension/azure/many/write-sequential-${i}${j}${k}.csv';

endloop

endloop

endloop

# Read it back as a sum: n*(n+1) / 2
query I 
SELECT sum(id) FROM '${proto}://duckdblabs-write-testing/extension/azure/many/write-sequential-*.csv';
----
499500


## Now go for some bigger files
statement ok
CREATE TABLE IF NOT EXISTS lineitem AS FROM './data/l.parquet';

# check source query
query I
SELECT sum(l_orderkey) FROM lineitem;
----
1802759573

# Now the big copy, csv file
statement ok
COPY (FROM lineitem CROSS JOIN generate_series(1, 10))
TO '${proto}://duckdblabs-write-testing/extension/azure/big/lineitem.csv';

# and confirm result from copy
query I
SELECT sum(l_orderkey) FROM '${proto}://duckdblabs-write-testing/extension/azure/big/lineitem.csv';
----
18027595730

# Finale: Same but different -- csv -> parquet, and hive partitioned
statement ok
COPY (FROM lineitem CROSS JOIN generate_series(1, 10))
TO '${proto}://duckdblabs-write-testing/extension/azure/big/lineitem-partitioned' (
	FORMAT 'parquet',
	PARTITION_BY (l_returnflag, l_linestatus),
	OVERWRITE_OR_IGNORE true
);

# and confirm result from copy
query I
SELECT sum(l_orderkey) 
FROM read_parquet(
	'${proto}://duckdblabs-write-testing/extension/azure/big/lineitem-partitioned/*/*/*.parquet', 
	HIVE_PARTITIONING = true
)
----
18027595730

endloop
