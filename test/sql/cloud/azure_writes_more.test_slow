# name: test/sql/cloud/azure_writes_more.test_slow
# description: test azure extension writes
# group: [cloud]

require azure

require parquet

require-env AZ_TEMP_DIR

require-env AZ_STORAGE_ACCOUNT

require-env AZURE_AUTH_ENV 1

statement ok
CREATE SECRET az1 (
    TYPE AZURE,
    PROVIDER CREDENTIAL_CHAIN,
    ACCOUNT_NAME '${AZ_STORAGE_ACCOUNT}'
)

## Many small writes - 1k
foreach i 0 1 2 3 4 5 6 7 8 9

foreach j 0 1 2 3 4 5 6 7 8 9

foreach k 0 1 2 3 4 5 6 7 8 9

statement ok
COPY (SELECT id: '${i}${j}${k}'::INT) 
TO 'az://${AZ_TEMP_DIR}/many/write-sequential-${i}${j}${k}.csv';

endloop

endloop

endloop

# Read it back as a sum: n*(n+1) / 2
query I 
SELECT sum(id) FROM 'az://${AZ_TEMP_DIR}/many/write-sequential-*.csv';
----
499500


## Some small writes - 10k, via 100 threads
concurrentloop i 0 100

statement ok
COPY (
	SELECT id: ('${i}'::INT * 100) + generate_series
	FROM generate_series(0, 99)
)
TO 'az://${AZ_TEMP_DIR}/many/write-concurrent-${i}.csv';

endloop

# Read it back as a sum: n*(n+1) / 2
query I 
SELECT sum(id) FROM 'az://${AZ_TEMP_DIR}/many/write-concurrent-*.csv';
----
49995000


## Now go for some bigger files
statement ok
CREATE TABLE lineitem AS FROM './data/l.parquet';

# check source query
query I
SELECT sum(l_orderkey) FROM lineitem;
----
1802759573

# Now the big copy, csv file
statement ok
COPY (FROM lineitem CROSS JOIN generate_series(1, 10))
TO 'az://${AZ_TEMP_DIR}/big/lineitem.csv';

# and confirm result from copy
query I
SELECT sum(l_orderkey) FROM 'az://${AZ_TEMP_DIR}/big/lineitem.csv';
----
18027595730

# Finale: Same but different -- csv -> parquet, and hive partitioned
statement ok
COPY (FROM lineitem CROSS JOIN generate_series(1, 10))
TO 'az://${AZ_TEMP_DIR}/big/lineitem-partitioned' (
	FORMAT 'parquet',
	PARTITION_BY (l_returnflag, l_linestatus),
	OVERWRITE_OR_IGNORE true
);

# and confirm result from copy
query I
SELECT sum(l_orderkey) 
FROM read_parquet(
	'az://${AZ_TEMP_DIR}/big/lineitem-partitioned/*/*/*.parquet', 
	HIVE_PARTITIONING = true
)
----
18027595730
